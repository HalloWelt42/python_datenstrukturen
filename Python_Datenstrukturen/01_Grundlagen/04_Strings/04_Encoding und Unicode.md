---
tags: [python, datenstrukturen, strings, encoding, unicode, utf8]
created: 2025-10-01
---

‚Üê [[03_Formatierung (f-strings)|Formatierung (f-strings)]] | [[INDEX|üìë Index]] | [[05_String Performance|String Performance]] ‚Üí

# Encoding und Unicode

> [!tip] Lernziel
> Du verstehst Unicode, verschiedene Encodings, und wie Python mit Text-Encoding umgeht

## Unicode Grundlagen

**Unicode** ist ein Standard, der jedem Zeichen eine eindeutige Nummer (Code Point) zuweist. Python 3 Strings sind **Unicode-Strings**.

```python
# Python 3 Strings sind Unicode
text = "Hello ‰∏ñÁïå üåç"
print(text)  # Funktioniert mit allen Unicode-Zeichen

# Jedes Zeichen hat einen Code Point
print(ord('A'))        # 65
print(ord('‚Ç¨'))        # 8364
print(ord('üêç'))       # 128013

# Von Code Point zu Zeichen
print(chr(65))         # A
print(chr(8364))       # ‚Ç¨
print(chr(128013))     # üêç

# Unicode Name
import unicodedata
print(unicodedata.name('‚Ç¨'))  # EURO SIGN
print(unicodedata.name('üêç'))  # SNAKE
```

## Encoding vs. Decoding

**Encoding** = Unicode String ‚Üí Bytes
**Decoding** = Bytes ‚Üí Unicode String

```python
# String ist Unicode
text = "Hallo Welt"
print(type(text))  # <class 'str'>

# Encode zu Bytes
bytes_utf8 = text.encode('utf-8')
print(bytes_utf8)       # b'Hallo Welt'
print(type(bytes_utf8)) # <class 'bytes'>

# Decode zur√ºck zu String
text_wieder = bytes_utf8.decode('utf-8')
print(text_wieder)      # Hallo Welt
print(type(text_wieder)) # <class 'str'>
```

## UTF-8 Encoding

**UTF-8** ist das Standard-Encoding in Python 3 und im Web.

```python
text = "Caf√© ‚òï"

# UTF-8 Encoding (variable L√§nge: 1-4 Bytes pro Zeichen)
utf8_bytes = text.encode('utf-8')
print(utf8_bytes)  # b'Caf\xc3\xa9 \xe2\x98\x95'
print(len(text))       # 6 (6 Unicode-Zeichen)
print(len(utf8_bytes)) # 9 (9 Bytes)

# ASCII: 1 Byte
# √©: 2 Bytes (C3 A9)
# ‚òï: 3 Bytes (E2 98 95)

# UTF-8 kann alle Unicode-Zeichen darstellen
emoji_text = "Python üêç ist toll! üéâ"
print(emoji_text.encode('utf-8'))
```

## Andere Encodings

### ASCII

```python
# ASCII (7-bit, nur 0-127)
text = "Hello"
ascii_bytes = text.encode('ascii')
print(ascii_bytes)  # b'Hello'

# Nicht-ASCII Zeichen funktionieren nicht
text_german = "Gr√º√üe"
try:
    ascii_bytes = text_german.encode('ascii')
except UnicodeEncodeError as e:
    print(f"Fehler: {e}")
    # 'ascii' codec can't encode character '\xfc'

# Mit Fehlerbehandlung
ascii_bytes = text_german.encode('ascii', errors='ignore')
print(ascii_bytes)  # b'Gre' (√º weggelassen)

ascii_bytes = text_german.encode('ascii', errors='replace')
print(ascii_bytes)  # b'Gr??e' (√º durch ? ersetzt)

ascii_bytes = text_german.encode('ascii', errors='xmlcharrefreplace')
print(ascii_bytes)  # b'Gr&#252;&#223;e' (XML entities)
```

### Latin-1 (ISO-8859-1)

```python
# Latin-1 (8-bit, nur westeurop√§ische Zeichen)
text = "Caf√©"
latin1_bytes = text.encode('latin-1')
print(latin1_bytes)  # b'Caf\xe9'
print(len(latin1_bytes))  # 4 (jedes Zeichen = 1 Byte)

# Aber keine Emojis oder asiatische Zeichen
emoji_text = "‚òï"
try:
    emoji_text.encode('latin-1')
except UnicodeEncodeError:
    print("Emoji nicht in Latin-1!")
```

### UTF-16 und UTF-32

```python
text = "Hello üåç"

# UTF-16 (2 oder 4 Bytes pro Zeichen)
utf16_bytes = text.encode('utf-16')
print(utf16_bytes)
print(len(utf16_bytes))  # Gr√∂√üer als UTF-8

# UTF-32 (immer 4 Bytes pro Zeichen)
utf32_bytes = text.encode('utf-32')
print(utf32_bytes)
print(len(utf32_bytes))  # Am gr√∂√üten

# Vergleich
print(f"UTF-8:  {len(text.encode('utf-8'))} bytes")
print(f"UTF-16: {len(text.encode('utf-16'))} bytes")
print(f"UTF-32: {len(text.encode('utf-32'))} bytes")
```

### Windows-1252 (CP1252)

```python
# Windows Standard-Encoding (Westeuropa)
text = "Caf√©"
windows_bytes = text.encode('cp1252')
print(windows_bytes)  # b'Caf\xe9'

# √Ñhnlich wie Latin-1 aber mit mehr Zeichen
# Enth√§lt z.B. ‚Ç¨, aber keine Emojis
```

## Fehlerbehandlung beim Encoding/Decoding

### Encoding Fehler

```python
text = "Python üêç"

# errors='strict' (default) - wirft Exception
try:
    text.encode('ascii')
except UnicodeEncodeError as e:
    print(f"Strict: {e}")

# errors='ignore' - √ºberspringt problematische Zeichen
print(text.encode('ascii', errors='ignore'))  # b'Python '

# errors='replace' - ersetzt mit ?
print(text.encode('ascii', errors='replace'))  # b'Python ?'

# errors='xmlcharrefreplace' - XML entities
print(text.encode('ascii', errors='xmlcharrefreplace'))
# b'Python &#128013;'

# errors='backslashreplace' - escape sequences
print(text.encode('ascii', errors='backslashreplace'))
# b'Python \\U0001f40d'

# errors='namereplace' - Unicode Namen
print(text.encode('ascii', errors='namereplace'))
# b'Python \\N{SNAKE}'
```

### Decoding Fehler

```python
# Falsche Bytes
falsche_bytes = b'\xff\xfe'

# errors='strict' (default) - wirft Exception
try:
    falsche_bytes.decode('utf-8')
except UnicodeDecodeError as e:
    print(f"Strict: {e}")

# errors='ignore' - √ºberspringt invalide Bytes
print(falsche_bytes.decode('utf-8', errors='ignore'))  # ''

# errors='replace' - ersetzt mit ÔøΩ
print(falsche_bytes.decode('utf-8', errors='replace'))  # ÔøΩÔøΩ

# errors='backslashreplace'
print(falsche_bytes.decode('utf-8', errors='backslashreplace'))
# \\xff\\xfe
```

## Dateien und Encoding

### Dateien lesen/schreiben

```python
# Schreiben mit UTF-8 (empfohlen!)
text = "Caf√© ‚òï Python üêç"
with open('test.txt', 'w', encoding='utf-8') as f:
    f.write(text)

# Lesen mit UTF-8
with open('test.txt', 'r', encoding='utf-8') as f:
    content = f.read()
    print(content)  # Caf√© ‚òï Python üêç

# ‚ö†Ô∏è Ohne encoding (plattformabh√§ngig!)
# Windows: cp1252, Linux/Mac: utf-8
# with open('test.txt', 'w') as f:  # Gef√§hrlich!
#     f.write(text)

# Explizit anderes Encoding
with open('test_latin.txt', 'w', encoding='latin-1') as f:
    f.write("Caf√©")  # Funktioniert (√© ist in Latin-1)
```

### Encoding-Erkennung

```python
# Encoding einer Datei erkennen mit chardet
try:
    import chardet
    
    with open('unknown.txt', 'rb') as f:
        raw_data = f.read()
        result = chardet.detect(raw_data)
        encoding = result['encoding']
        confidence = result['confidence']
        
        print(f"Erkanntes Encoding: {encoding} ({confidence*100:.1f}% sicher)")
        
        # Mit erkanntem Encoding lesen
        text = raw_data.decode(encoding)
except ImportError:
    print("chardet nicht installiert: pip install chardet")
```

## BOM (Byte Order Mark)

```python
# BOM in UTF-8 (optional, oft bei Windows)
text = "Hello"

# UTF-8 ohne BOM (standard)
utf8 = text.encode('utf-8')
print(utf8)  # b'Hello'

# UTF-8 mit BOM (utf-8-sig)
utf8_bom = text.encode('utf-8-sig')
print(utf8_bom)  # b'\xef\xbb\xbfHello'

# BOM entfernen beim Lesen
with open('file_with_bom.txt', 'r', encoding='utf-8-sig') as f:
    content = f.read()  # BOM wird automatisch entfernt

# UTF-16 hat immer BOM
utf16 = text.encode('utf-16')
print(utf16)  # b'\xff\xfeH\x00e\x00l\x00l\x00o\x00' (LE)
```

## Unicode Normalisierung

```python
import unicodedata

# Verschiedene Darstellungen des gleichen Zeichens
# Composed (√© als ein Zeichen)
text1 = "caf√©"
# Decomposed (e + combining accent)
text2 = "cafe\u0301"

print(text1)  # caf√©
print(text2)  # caf√© (sieht gleich aus!)
print(text1 == text2)  # False (verschiedene Bytes!)

print(len(text1))  # 4
print(len(text2))  # 5 (e + accent = 2 Zeichen)

# Normalisierung
# NFC (Canonical Composition) - kombiniert
nfc1 = unicodedata.normalize('NFC', text1)
nfc2 = unicodedata.normalize('NFC', text2)
print(nfc1 == nfc2)  # True

# NFD (Canonical Decomposition) - trennt
nfd1 = unicodedata.normalize('NFD', text1)
nfd2 = unicodedata.normalize('NFD', text2)
print(nfd1 == nfd2)  # True

# F√ºr Vergleiche: Immer normalisieren!
def unicode_vergleich(s1, s2):
    return unicodedata.normalize('NFC', s1) == unicodedata.normalize('NFC', s2)

print(unicode_vergleich("caf√©", "cafe\u0301"))  # True
```

## Praktische Anwendungen

### Anwendung 1: Slugify f√ºr URLs

```python
import unicodedata
import re

def slugify(text):
    """Erstellt URL-sicheren Slug"""
    # Unicode zu ASCII (√§hnliche Zeichen)
    text = unicodedata.normalize('NFKD', text)
    text = text.encode('ascii', 'ignore').decode('ascii')
    
    # Lowercase
    text = text.lower()
    
    # Nur alphanumerisch und Spaces
    text = re.sub(r'[^a-z0-9\s-]', '', text)
    
    # Spaces zu Bindestrichen
    text = re.sub(r'[\s]+', '-', text)
    
    # Mehrfache Bindestriche zu einem
    text = re.sub(r'-+', '-', text)
    
    # Trim Bindestriche
    text = text.strip('-')
    
    return text

# Tests
print(slugify("Caf√© Paris"))           # cafe-paris
print(slugify("√úber uns & Kontakt"))   # uber-uns-kontakt
print(slugify("S√£o Paulo, Brasil!"))   # sao-paulo-brasil
print(slugify("Êó•Êú¨Ë™û"))               # '' (keine ASCII-√Ñquivalente)
```

### Anwendung 2: Text-Bereinigung

```python
def bereinige_text(text):
    """Entfernt unsichtbare und problematische Zeichen"""
    # Verschiedene Whitespace-Zeichen zu normalem Space
    text = ' '.join(text.split())
    
    # Zero-width Zeichen entfernen
    zero_width = [
        '\u200b',  # Zero width space
        '\u200c',  # Zero width non-joiner
        '\u200d',  # Zero width joiner
        '\ufeff',  # Zero width no-break space
    ]
    for char in zero_width:
        text = text.replace(char, '')
    
    # Steuerzeichen entfernen (au√üer newline/tab)
    text = ''.join(
        char for char in text
        if not unicodedata.category(char).startswith('C')
        or char in '\n\t'
    )
    
    return text

dirty = "Test\u200b\u200c  mit   seltsamen\ufeff   Zeichen"
clean = bereinige_text(dirty)
print(f"'{clean}'")  # 'Test mit seltsamen Zeichen'
```

### Anwendung 3: Email-Validierung (Unicode)

```python
def ist_g√ºltige_email(email):
    """Validiert Email mit Unicode-Support"""
    # Normalisieren
    import unicodedata
    email = unicodedata.normalize('NFC', email)
    
    # Einfache Pr√ºfung
    if '@' not in email:
        return False
    
    local, domain = email.rsplit('@', 1)
    
    # Local part kann Unicode enthalten
    if not local:
        return False
    
    # Domain sollte Punycode sein oder ASCII
    try:
        domain.encode('ascii')
    except UnicodeEncodeError:
        # Internationalized Domain Names (IDN)
        try:
            domain.encode('idna')
        except:
            return False
    
    return True

# Tests
print(ist_g√ºltige_email("user@example.com"))      # True
print(ist_g√ºltige_email("user@m√ºnchen.de"))       # True (IDN)
print(ist_g√ºltige_email("Áî®Êà∑@‰æã„Åà.jp"))           # True
```

### Anwendung 4: Punycode (IDN)

```python
# Internationalized Domain Names
domain = "m√ºnchen.de"

# Zu Punycode (ASCII-safe)
punycode = domain.encode('idna').decode('ascii')
print(punycode)  # xn--mnchen-3ya.de

# Zur√ºck
original = punycode.encode('ascii').decode('idna')
print(original)  # m√ºnchen.de

# Vollst√§ndige URL
url = "https://www.caf√©.com/√ºber-uns"
# Domain-Teil zu Punycode
parts = url.split('/')
domain_part = parts[2]
punycode_domain = domain_part.encode('idna').decode('ascii')
parts[2] = punycode_domain
safe_url = '/'.join(parts)
print(safe_url)  # https://www.xn--caf-dma.com/√ºber-uns
```

### Anwendung 5: CSV mit verschiedenen Encodings

```python
import csv

def lese_csv_auto_encoding(filename):
    """Liest CSV mit automatischer Encoding-Erkennung"""
    # Probiere verschiedene Encodings
    encodings = ['utf-8', 'utf-8-sig', 'latin-1', 'cp1252']
    
    for encoding in encodings:
        try:
            with open(filename, 'r', encoding=encoding) as f:
                reader = csv.reader(f)
                data = list(reader)
                print(f"Erfolgreich mit {encoding}")
                return data
        except (UnicodeDecodeError, UnicodeError):
            continue
    
    raise ValueError(f"Konnte {filename} mit keinem Encoding lesen")

# Schreibe CSV mit UTF-8 (empfohlen!)
def schreibe_csv_utf8(filename, data):
    """Schreibt CSV mit UTF-8 und BOM f√ºr Excel-Kompatibilit√§t"""
    with open(filename, 'w', encoding='utf-8-sig', newline='') as f:
        writer = csv.writer(f)
        writer.writerows(data)
```

## H√§ufige Fallstricke

### Fallstrick 1: Bytes vs. String verwechseln

```python
# ‚ùå Bytes und Strings mischen
text = "Hallo"
bytes_data = b"Welt"

try:
    result = text + bytes_data  # TypeError!
except TypeError as e:
    print(f"Fehler: {e}")

# ‚úÖ Erst dekodieren
result = text + bytes_data.decode('utf-8')
print(result)  # HalloWelt
```

### Fallstrick 2: Encoding vergessen bei Dateien

```python
# ‚ùå Ohne encoding (plattformabh√§ngig!)
# Windows: cp1252, Linux: utf-8
with open('test.txt', 'w') as f:
    f.write("Caf√©")  # Kann auf Windows anders aussehen!

# ‚úÖ Immer encoding angeben
with open('test.txt', 'w', encoding='utf-8') as f:
    f.write("Caf√©")
```

### Fallstrick 3: String-L√§nge vs. Byte-L√§nge

```python
text = "üêç"
print(len(text))                    # 1 (ein Unicode-Zeichen)
print(len(text.encode('utf-8')))    # 4 (4 Bytes in UTF-8)

# F√ºr Datenbank CHAR(10) - welche L√§nge?
text = "Helloüêçüêçüêç"
print(len(text))                    # 8 Zeichen
print(len(text.encode('utf-8')))    # 17 Bytes!
```

### Fallstrick 4: Encoding-Mismatch

```python
# Schreiben mit einem Encoding, Lesen mit anderem
text = "Caf√©"

# Schreiben mit UTF-8
with open('test.txt', 'wb') as f:
    f.write(text.encode('utf-8'))

# ‚ùå Lesen mit Latin-1
with open('test.txt', 'rb') as f:
    bytes_data = f.read()
    falsch = bytes_data.decode('latin-1')
    print(falsch)  # Caf√É¬© (Mojibake!)

# ‚úÖ Mit richtigem Encoding
with open('test.txt', 'rb') as f:
    bytes_data = f.read()
    richtig = bytes_data.decode('utf-8')
    print(richtig)  # Caf√©
```

### Fallstrick 5: Unicode Normalisierung vergessen

```python
# Verschiedene Darstellungen
text1 = "Caf√©"        # √© als ein Zeichen (U+00E9)
text2 = "Cafe\u0301"  # e + accent (U+0065 + U+0301)

# ‚ùå Direkter Vergleich schl√§gt fehl
print(text1 == text2)  # False!

# ‚úÖ Nach Normalisierung
import unicodedata
nfc1 = unicodedata.normalize('NFC', text1)
nfc2 = unicodedata.normalize('NFC', text2)
print(nfc1 == nfc2)  # True
```

## Performance-Hinweise

### Encoding/Decoding Performance

```python
import timeit

text = "Hello World" * 1000

# UTF-8 (am schnellsten f√ºr ASCII)
t1 = timeit.timeit(lambda: text.encode('utf-8'), number=10000)
print(f"UTF-8:  {t1:.4f}s")

# Latin-1 (auch schnell)
t2 = timeit.timeit(lambda: text.encode('latin-1'), number=10000)
print(f"Latin1: {t2:.4f}s")

# UTF-16 (langsamer)
t3 = timeit.timeit(lambda: text.encode('utf-16'), number=10000)
print(f"UTF-16: {t3:.4f}s")

# UTF-8 ist meist am schnellsten und flexibelsten
```

### Caching bei wiederholtem Encoding

```python
# ‚ùå Wiederholtes Encoding
def langsam(text, n):
    for _ in range(n):
        bytes_data = text.encode('utf-8')  # Jedes Mal neu!
        # ... verarbeite bytes_data

# ‚úÖ Einmal encodieren
def schnell(text, n):
    bytes_data = text.encode('utf-8')  # Nur einmal!
    for _ in range(n):
        # ... verarbeite bytes_data
        pass
```

## √úbungsaufgaben

### √úbung 1: Encoding-Detektor (Leicht)

Erkenne das Encoding einer Datei durch Probieren.

**Erwarteter Output:**
```python
encoding = erkenne_encoding('datei.txt')
print(encoding)  # 'utf-8' oder 'latin-1' etc.
```

### √úbung 2: Unicode-Info (Mittel)

Zeige Informationen √ºber Unicode-Zeichen.

**Erwarteter Output:**
```python
info = unicode_info('√©')
# {
#   'character': '√©',
#   'codepoint': 'U+00E9',
#   'name': 'LATIN SMALL LETTER E WITH ACUTE',
#   'category': 'Lowercase Letter',
#   'utf8': 'C3 A9'
# }
```

### √úbung 3: Encoding-Konverter (Mittel)

Konvertiere Datei von einem Encoding zu anderem.

**Erwarteter Output:**
```python
konvertiere_encoding('input.txt', 'latin-1', 'utf-8', 'output.txt')
# Datei erfolgreich konvertiert
```

### √úbung 4: Text-Sanitizer (Schwer)

Bereinige Text von problematischen Unicode-Zeichen.

**Erwarteter Output:**
```python
text = "Test\u200b mit\ufeff Zero-Width"
clean = sanitize_text(text)
# "Test mit Zero-Width"
```

### √úbung 5: Multilingual Slug (Schwer)

Erstelle URL-Slugs f√ºr verschiedene Sprachen.

**Erwarteter Output:**
```python
print(multilingual_slug("–ü—Ä–∏–≤–µ—Ç –º–∏—Ä"))  # privet-mir (Transliteration)
print(multilingual_slug("„Åì„Çì„Å´„Å°„ÅØ"))    # konnichiwa (Romanisierung)
print(multilingual_slug("Caf√© Paris"))  # cafe-paris
```

## Python-Dokumentation

üìö **Offizielle Ressourcen:**
- [Unicode HOWTO](https://docs.python.org/3/howto/unicode.html) - Umfassender Unicode-Guide
- [codecs Module](https://docs.python.org/3/library/codecs.html) - Encoding/Decoding
- [unicodedata Module](https://docs.python.org/3/library/unicodedata.html) - Unicode-Datenbank
- [String encode/decode](https://docs.python.org/3/library/stdtypes.html#str.encode) - Encoding-Methoden

## Zusammenfassung

### Wichtigste Punkte

1. **Unicode Basics:**
   - Python 3 Strings sind Unicode
   - Jedes Zeichen hat Code Point
   - `ord()` und `chr()` f√ºr Konvertierung

2. **Encoding:**
   - String ‚Üí Bytes: `encode()`
   - Bytes ‚Üí String: `decode()`
   - UTF-8 ist Standard (empfohlen!)

3. **Wichtige Encodings:**
   - UTF-8: Universal, variable L√§nge
   - ASCII: Nur 0-127
   - Latin-1: Westeurop√§isch, 1 Byte
   - UTF-16/32: Mehr Speicher

4. **Fehlerbehandlung:**
   - `errors='strict'`: Exception (default)
   - `errors='ignore'`: √úberspringen
   - `errors='replace'`: Ersetzen mit ?/ÔøΩ

5. **Best Practices:**
   - ‚úÖ Immer UTF-8 f√ºr Dateien
   - ‚úÖ Encoding explizit angeben
   - ‚úÖ Normalisieren f√ºr Vergleiche
   - ‚úÖ Bytes und Strings nicht mischen
   - ‚ö†Ô∏è L√§nge: Zeichen ‚â† Bytes!

## Verwandte Themen

- [[01_String Basics|String Basics]] - String-Grundlagen
- [[02_String Methoden|String Methoden]] - String-Operationen
- [[03_Formatierung (f-strings)|Formatierung]] - String-Formatierung
- [[05_String Performance|Performance]] - Optimierungen
- [[06_√úbungen Strings|√úbungen]] - Mehr Praxis

---

‚Üê [[03_Formatierung (f-strings)|Formatierung (f-strings)]] | [[INDEX|üìë Index]] | [[05_String Performance|String Performance]] ‚Üí